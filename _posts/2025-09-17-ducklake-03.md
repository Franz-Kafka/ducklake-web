---
layout: post
title: "Updates in DuckLake 0.3"
author: "Guillermo Sanchez"
thumb: "/images/blog/thumbs/ducklake-0-2.svg"
image: "/images/blog/thumbs/ducklake-0-2.png"
excerpt: "We are releasing the updated DuckLake 0.3 extension and specification with several new fatures."
---

**TL;DR: We are releasing version 0.3 of the DuckLake Specification and the DuckLake DuckDB extension.**

The focus of the team in the last two months has been around making DuckLake as robust as possible. Besides this, we have implemented a bunch of features that will make the experience of running DuckLake even smoother. In this post we will discuss some of them.

### Interoperability with Iceberg

Powered by the DuckDB Iceberg extension, it is possible to copy data from DuckLake to Iceberg.

```sql  
-- Basic Iceberg setup from [https://github.com/duckdb/duckdb-iceberg/blob/main/scripts/start-rest-catalog.sh](https://github.com/duckdb/duckdb-iceberg/blob/main/scripts/start-rest-catalog.sh)  
CREATE SECRET (  
      TYPE S3,  
      KEY_ID 'admin',  
      SECRET 'password',  
      ENDPOINT '127.0.0.1:9000',  
      URL_STYLE 'path',  
      USE_SSL false  
  );  
ATTACH '' AS iceberg_datalake (  
      TYPE iceberg,  
      CLIENT_ID 'admin',  
      CLIENT_SECRET 'password',  
      ENDPOINT 'http://127.0.0.1:8181'  
  );  
ATTACH 'ducklake:my_ducklake.ducklake' AS ducklake (DATA_PATH 'data/');  
CREATE SCHEMA iceberg_datalake.default;  
CREATE TABLE iceberg_datalake.default.iceberg_table AS SELECT range a FROM range(4);  
COPY FROM DATABASE iceberg_datalake TO ducklake;  
```

Copying from DuckLake to Iceberg also works, given that the schemas are already created in Iceberg.

```sql  
-- Assuming Iceberg catalog is empty since the COPY command does not replace tables  
CREATE SCHEMA iceberg_datalake.main;  
CREATE TABLE ducklake.default.ducklake_table AS SELECT range a FROM range(4);  
COPY FROM DATABASE ducklake TO iceberg_datalake;  
```

These examples are data copies, which means that only data is ported from Iceberg to DuckLake and vice versa. Metadata only copies are being worked on.

### MERGE INTO Statement

Since DuckDB 1.4 supports `MERGE INTO` this functionality can also be used in the DuckLake extension since this release. This is a very common statement in OLAP systems that do not support primary keys but still want to support upserting (i.e. `UPDATE` plus `INSERT`) functionality.

```sql  
CREATE TABLE Stock(item_id INTEGER, balance INTEGER);  
INSERT INTO Stock VALUES (10, 2200), (20, 1900);

WITH new_stocks(item_id, volume) AS (VALUES (20, 2200), (30, 1900))  
  MERGE INTO Stock USING new_stocks USING (item_id)  
  WHEN MATCHED THEN UPDATE SET balance = balance + volume  
  WHEN NOT MATCHED THEN INSERT VALUES (new_stocks.item_id, new_stocks.volume);  
FROM Stock;  
```  
```text  
┌─────────┬─────────┐  
│ item_id │ balance │  
│  int32  │  int32  │  
├─────────┼─────────┤  
│      10 │    2200 │  
│      20 │    4100 │  
│      30 │    1900 │  
└─────────┴─────────┘  
```

`MERGE INTO` also supports more complex conditions and `DELETE` statements.

```sql  
WITH deletes(item_id, delete_threshold) AS (VALUES (10, 3000))  
    MERGE INTO Stock USING deletes USING (item_id)  
    WHEN MATCHED AND balance < delete_threshold THEN DELETE;  
FROM Stock;  
```

```text  
┌─────────┬─────────┐  
│ item_id │ balance │  
│  int32  │  int32  │  
├─────────┼─────────┤  
│      20 │    4100 │  
│      30 │    1900 │  
└─────────┴─────────┘  
```

### CHECKPOINT Statement

DuckLake now also supports the `CHEKPOINT` statement. In DuckLake, this statement runs a series of maintenance functions in a sequential order. This includes flushing inlined data, expiring snapshots, compacting files and rewriting heavily deleted files as well as cleaning up old or orphaned files. The CHECKPOINT statement can be configured via some global options that can be set via the `ducklake.set_option` function. 

```sql  
ATTACH ‘ducklake:my_ducklake.ducklake’ AS my_ducklake;  
USE my_ducklake;  
CHECKPOINT;  
```

### Support for Geometry Types

Geometry types are now supported in DuckLake. This means that you can now use most of the functionality in the spatial extension of DuckDB with DuckLake as a backend. All supported geometry primitives are [documented here]([https://ducklake.select/docs/preview/specification/data_types#geometry-types](https://ducklake.select/docs/preview/specification/data_types#geometry-types)).

```sql  
INSTALL spatial;  
LOAD spatial;

ATTACH 'ducklake: my_ducklake.ducklake' AS ducklake;  
CREATE TABLE geometry_table (polygons GEOMETRY);  
INSERT INTO geometry_table VALUES ('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))');  
SELECT   
polygons,   
ST_Area(polygons) AS area   
FROM geometry_table;  
```  
```text  
┌─────────────────────────────────────┬────────┐  
│              polygons               │  area  │  
│              geometry               │ double │  
├─────────────────────────────────────┼────────┤  
│ POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0)) │  1.0   │  
└─────────────────────────────────────┴────────┘  
```

### Faster Inserts

In certain cases where the output of each thread in a certain query is of a desired size (i.e. not too small), setting the option `per_thread_output` will most likely speed up the insertion process in DuckLake. If you have a large number of threads and the output of each thread is not very significant, using `per_thread_output` will likely not improve the performance that much and will generate an undesired amount of small files that will hinder read performance. You can benefit a lot from this option if you are running a setup where DuckDB is in an EC2 instance and has very big network bandwidth (up to 100Gbps) to write to S3.

In the following benchmark we did find ~25% improvement when enabling `per_thread_output`:

```sql  
.timer on  
ATTACH 'ducklake: my_ducklake.ducklake' AS ducklake;  
CREATE TABLE sample_table AS SELECT * FROM range(1_000_000_000);  
-- 4.5 seconds  
CREATE TABLE slow_copy AS SELECT * FROM sample_table;

-- enable the option  
CALL ducklake.set_option('per_thread_output', true);

-- 3.4 seconds  
CREATE TABLE fast_copy AS SELECT * FROM sample_table;  
```

### Migration

You don’t have to worry about migrating from specs manually. The moment you attach to your existing DuckLake with the new extension installed, the extension will update the metadata catalog to the 0.3 version.

### Other

In the last month, we have also been preparing some guides to help you adopt DuckLake and cover some topics that are not features of DuckLake but will help you run your DuckLake smoothly.

- [Migrations](https://ducklake.select/docs/preview/duckdb/migrations/duckdb_to_ducklake)
- [Backups and Recovery](https://ducklake.select/docs/preview/duckdb/guides/backups_and_recovery)   
- [Access Control](https://ducklake.select/docs/preview/duckdb/guides/access_control)

We have also enabled some functionality to be able to [author commits](https://ducklake.select/docs/preview/duckdb/usage/snapshots#adding-a-commit-message-to-a-snapshot) whenever you execute a new transaction in DuckLake. This functionality helps greatly with audit needs.
